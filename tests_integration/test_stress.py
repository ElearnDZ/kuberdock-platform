import collections
import logging
import uuid
import json
import time
from tests_integration.lib.pipelines import pipeline
from tests_integration.lib.exceptions import KDIsNotSane
from tests_integration.lib.load_testing_utils import check_sanity
from tests_integration.lib.utils import (pod_factory, loglevel,
                                         log_workload, POD_STATUSES)
from tests_integration.lib.cluster_utils import set_kubelet_multipliers

LOG = logging.getLogger(__name__)
LOG.setLevel(logging.DEBUG)

BATCH_SIZE = 10
STATISTIC_FAKE_PODS_COUNT = 1000
STATISTIC_COLLECTION_PERIOD = 60  # sec
PODS_ROLLBACK_COUNT = 50
RECOVERY_TIMEOUT = 5  # sec


INFLUXDB_URL = "http://127.0.0.1:8086"


def influxdb_query(cluster, q):
    cmd = ('curl -sG --fail {0}  --data-urlencode "q={1}"'
           ' --data-urlencode "db=k8s"').format(
        INFLUXDB_URL + '/query', q)
    _, result_raw, _ = cluster.ssh_exec("master", cmd)
    return result_raw


def collect_statistic_data(pod):
    """ Do some work on PA and get statistic produced
    by this period
    """
    # Collect data
    query = "select * from /.*/ where time > now() - {0:.0f}s and time <= now()".format(
        STATISTIC_COLLECTION_PERIOD
    )
    result_raw = influxdb_query(pod.cluster, query)
    result = json.loads(result_raw)['results'][0]['series']
    data = []
    for rec in result:
        values = dict((k, v) for k, v in
                      zip(rec['columns'], rec['values'][0]) if v is not None)
        data.append({
            "name": rec['name'],
            "timestamp": values.pop('time'),
            "value": values.pop('value'),
            "data": values
        })
    return data


def write_statistic_data(cluster, pod_id, data):
    records = []
    for rec in data:
        rec['data']['pod_id'] = pod_id
        raw_data = ','.join('{0}={1}'.format(k, v.replace(",", "\\,"))
                            for k, v in rec['data'].items())
        records.append('{0},{1} value={2} {3}'.format(
            rec['name'], raw_data, rec['value'], int(time.time())))
    url = "{0}/write?db=k8s".format(INFLUXDB_URL)
    cmd = "curl --fail -is -XPOST '{0}' --data-binary '{1}'".format(
        url, '\n'.join(records))
    with loglevel(logging.INFO):
        _, result_raw, _ = cluster.ssh_exec("master", cmd)


def influx_fake_pod():
    """ TODO: It should create some initial data for pod.
    Or may be not.
    """
    return str(uuid.uuid4())


def check_recovery(cluster, pods):
    """ Wait until KD back from overload.
    """
    time.sleep(3)  # Get some rest after overload.
    # We will wait for a one minute.
    wait_time = 60
    wait_stop = time.time() + wait_time
    while wait_stop > time.time():
        try:
            # We do not need any debug stdout from here
            # because of huge logs generated by those part
            with loglevel(logging.INFO):
                check_sanity(cluster, pods)
            LOG.info("KD is up now. It took "
                     "{}s to recover.".format(
                         wait_time - (wait_stop - time.time())))
            # KD is sane so we can go further
            return
        except KDIsNotSane as err:
            time.sleep(RECOVERY_TIMEOUT)
            LOG.info("KD is not up yet: {}".format(repr(err)))
            continue
    raise KDIsNotSane("Kuberdock does not return from down in 60 seconds.")


get_pod = pod_factory("hub.kuberdock.com/webhook:v1",
                      start=True, wait_for_status=POD_STATUSES.running,
                      kube_type='Tiny')


def get_pods_batch(cluster, public=False):
    return get_pod(cluster, num=BATCH_SIZE, open_all_ports=public)


def do_recovery(cluster, control_pods, pods_to_delete):
    """ At first try to check sanity again with no increased load.
    Then if it still fails try to delete a bunch of pods and check again.
    """
    try:
        check_recovery(cluster, control_pods)
    except KDIsNotSane as err:
        LOG.info("KD API not recovered: \n{0}\n.  Trying to delete "
                 "last created nodes and check again.".format(repr(err)))
        log_workload(cluster, "master")
        for pod in pods_to_delete:
            pod.delete()
        try:
            check_recovery(cluster, control_pods)
        except KDIsNotSane:
            LOG.info("KD API not recovered even after deletion "
                     "last {} pods.".format(PODS_ROLLBACK_COUNT))
            LOG.info(log_workload(cluster, "master"))
            raise


@pipeline("stress_testing")
def test_statistic_stress(cluster):
    LOG.debug("Create wordpress PA")
    description = ("Each iteration is an amount of statistics generated by "
                   "POST request to {} different wordpress pods.").format(
                       STATISTIC_FAKE_PODS_COUNT)
    pod = cluster.pods.create_pa('wordpress.yaml', wait_ports=True,
                                 wait_for_status=POD_STATUSES.running,
                                 healthcheck=True)
    LOG.debug("Do some load on wordpress")
    pod.gen_workload(STATISTIC_COLLECTION_PERIOD)
    LOG.debug("Collect statistic for later usage")
    statistic = collect_statistic_data(pod)
    fake_pods = [influx_fake_pod() for i in range(STATISTIC_FAKE_PODS_COUNT)]
    check_sanity(cluster, [pod, ])

    # Maximum duration of stress test is 1 hour
    wait_time = 60 * 60
    wait_stop = time.time() + wait_time
    iterations = 0

    try:
        while time.time() < wait_stop:
            LOG.debug("Generating statiscs for {0} pods.".format(
                STATISTIC_FAKE_PODS_COUNT))
            for fake in fake_pods:
                write_statistic_data(pod.cluster, fake, statistic)
            LOG.debug("Checking cluster API sanity")
            check_sanity(cluster, [pod, ])
            iterations += 1
    except KDIsNotSane:
        LOG.info("It took {0} iterations to degrade performance. {1}".format(
            iterations, description))
        check_recovery(cluster, [pod, ])
        LOG.debug("KD is up now.")
    LOG.info("{0} iterations passed sucessfully. {1} "
             "KD API (including statistics) response have been under 2 seconds"
             " all this time.".format(iterations, description))


@pipeline("stress_testing")
def test_pods_count_stress(cluster):
    cpu_mult = 30  # 480 kubes by CPU.
    ram_mult = 10  # 640 kubes by RAM.

    LOG.debug("Modifying multipliers.")
    set_kubelet_multipliers(cluster, cpu_mult, ram_mult)

    LOG.debug("Create first {0} control pods.".format(BATCH_SIZE))
    control_pods = get_pods_batch(cluster, public=True)
    LOG.debug("Initial check for control pods.")
    check_sanity(cluster, control_pods)
    LOG.debug("Starting stress test: adding new pods.")
    # TODO: rewrite it as a itertools.counter()
    counter = 0
    pod_buffer = collections.deque(maxlen=PODS_ROLLBACK_COUNT)
    description = "Each iteration is an creation of {} pods".format(BATCH_SIZE)
    try:
        while True:
            counter += 1
            pod_buffer.extend(get_pods_batch(cluster))
            with loglevel(logging.INFO):
                check_sanity(cluster, control_pods)
    except KDIsNotSane:
        LOG.info("It took {0} iterations to degrade performance. {1}".format(
            counter, description))
        do_recovery(cluster, control_pods, pod_buffer)
